{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eBrain examples: vim-2\n",
    "\n",
    "– by http://www.ccnlab.net ––\n",
    "\n",
    "\n",
    "This notebook demonstrates how an encoding model can be applied on real video data. We use the publically available vim-2 fMRI data set from the Gallant lab.\n",
    "\n",
    "Dataset available from https://crcns.org/data-sets\n",
    "Dataset info from https://crcns.org/files/data/vim-2/crcns-vim-2-readme.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Set your ebrain base directory ***\n",
    "import os\n",
    "os.chdir('/vol/ccnlab-scratch1/egrant/ebrain')\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "import skimage.transform\n",
    "from encoding_models.encoding_model import EncodingModel\n",
    "from feature_models.gabor_wavelet_pyramid import GaborWaveletPyramid\n",
    "from feature_models.convolutional_neural_network import CNN\n",
    "from feature_models.identity import Identity\n",
    "from response_models.kernel_ridge_regression import KernelRidgeRegression\n",
    "from encoding_models.ring_buffer import RingBuffer\n",
    "from matplotlib import pyplot as plt\n",
    "import tables\n",
    "import scipy.io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Import data from the VIM-2 dataset. We choose voxels from the region of interest (ROI) V1LH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing data (May take some time)\n",
      "Cleaning  data\n"
     ]
    }
   ],
   "source": [
    "print('Importing data (May take some time)')\n",
    "f = tables.open_file('/vol/ccnlab-scratch1/egrant/ebrain/Data/VIM2/VoxelResponses_subject1.mat')\n",
    "f.listNodes # Show all variables available\n",
    "data_train = f.get_node('/rt')[:]\n",
    "data_val = f.get_node('/rv')[:]\n",
    "roi = f.get_node('/roi/v1lh')[:].flatten()\n",
    "v1lh_idx = np.nonzero(roi==1)[0]\n",
    "V1LHresp_train = data_train[v1lh_idx].T\n",
    "V1LHresp_val = data_val[v1lh_idx].T\n",
    "Stimuli= h5py.File('/vol/ccnlab-scratch1/egrant/ebrain/Data/VIM2/Stimuli.mat')\n",
    "stim_train = Stimuli['st'][0::15,:,:,:].astype('float')/255.0 #15Hz\n",
    "stim_val = Stimuli['sv'][0::15,:,:,:].astype('float')/255.0 #15Hz\n",
    "#convert video to grayscale and resize\n",
    "stim_train=0.21*stim_train[:,0,:,:]+0.72*stim_train[:,1,:,:]+0.07*stim_train[:,2,:,:]\n",
    "stim_val=0.21*stim_val[:,0,:,:]+0.72*stim_val[:,1,:,:]+0.07*stim_val[:,2,:,:]\n",
    "stim_train=np.transpose(stim_train,(1,2,0))\n",
    "stim_val=np.transpose(stim_val,(1,2,0))\n",
    "stim_train = skimage.transform.resize(stim_train, (64, 64))\n",
    "stim_val = skimage.transform.resize(stim_val, (64, 64))\n",
    "stim_train=np.transpose(stim_train,(2,0,1))\n",
    "stim_val=np.transpose(stim_val,(2,0,1))\n",
    "#Reshape\n",
    "stim_train=np.reshape(stim_train,(stim_train.shape[0],stim_train.shape[1]*stim_train.shape[2]))\n",
    "stim_val=np.reshape(stim_val,(stim_val.shape[0],stim_val.shape[1]*stim_val.shape[2]))\n",
    "del Stimuli #Cleanup\n",
    "del f\n",
    "del data_train\n",
    "del data_val\n",
    "\n",
    "\n",
    "## Clean response data\n",
    "print('Cleaning  data')\n",
    "mask = (np.nan_to_num(V1LHresp_val) != 0 ).all(axis=0) | (np.nan_to_num(V1LHresp_train) != 0 ).all(axis=0)\n",
    "V1LHresp_train=V1LHresp_train[:,mask]\n",
    "V1LHresp_train[np.isnan(V1LHresp_train)]=0\n",
    "V1LHresp_val=V1LHresp_val[:,mask]\n",
    "V1LHresp_val[np.isnan(V1LHresp_val)]=0\n",
    "\n",
    "\n",
    "## Select n random voxels for demo set\n",
    "n_vox=V1LHresp_train.shape[1]   #set to 'V1LHresp_train.shape[1]' for all voxels\n",
    "np.random.seed(0)\n",
    "target_vox=np.random.randint(n_vox, size=n_vox)\n",
    "V1LHresp_train=V1LHresp_train[:,target_vox]\n",
    "V1LHresp_val=V1LHresp_val[:,target_vox]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now let's create, fit and predict with an EncodingModel object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating encoding model\n",
      "Training encoding model\n",
      "'TRAIN_LINEAR_KERNEL_RIDGE_REGRESSION (a / c): 1 / 1\n",
      "'TRAIN_LINEAR_KERNEL_RIDGE_REGRESSION (b / c): 1 / 490\n",
      "'TRAIN_LINEAR_KERNEL_RIDGE_REGRESSION (b / c): 2 / 490\n",
      "'TRAIN_LINEAR_KERNEL_RIDGE_REGRESSION (b / c): 3 / 490\n",
      "'...\n",
      "'TRAIN_LINEAR_KERNEL_RIDGE_REGRESSION (b / c): 487 / 490\n",
      "'TRAIN_LINEAR_KERNEL_RIDGE_REGRESSION (b / c): 488 / 490\n",
      "'TRAIN_LINEAR_KERNEL_RIDGE_REGRESSION (b / c): 489 / 490\n",
      "'TRAIN_LINEAR_KERNEL_RIDGE_REGRESSION (b / c): 490 / 490\n",
      "'TRAIN_LINEAR_KERNEL_RIDGE_REGRESSION (c / c): 1 / 1\n",
      "Predicting validation responses\n",
      "PREDICT_LINEAR_KERNEL_RIDGE_REGRESSION: 1 / 1\n"
     ]
    }
   ],
   "source": [
    "## Define encoding model\n",
    "print('Creating encoding model')\n",
    "#weights='/vol/ccnlab-scratch1/egrant/ebrain/Data/vgg16_weights.h5' #path to CNN weights\n",
    "#fm=CNN(weights) #CNN feature model\n",
    "fm=Identity() #Identity feature model\n",
    "#fm=GaborWaveletPyramid() #GWP feature model\n",
    "rm=KernelRidgeRegression() #Response model\n",
    "buff_size=5 #Size of ring buffer\n",
    "em=EncodingModel(fm,rm,buff_size)\n",
    "\n",
    "\n",
    "## Fit encoding model \n",
    "print('Training encoding model')\n",
    "em.fit(stim_train,V1LHresp_train)\n",
    "\n",
    "\n",
    "## Predict encoding model \n",
    "print('Predicting validation responses')\n",
    "V1LHresp_val_hat=em.predict(stim_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Align randomized stimuli with responses and remove insignificant voxels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Align responses with buffer features\n",
    "V1LHresp_val=V1LHresp_val[em.val_feature_idx[:,0]+buff_size-1,:] \n",
    "\n",
    "\n",
    "## Remove insignificant voxels\n",
    "significant_vox=em.rm.H_0==False\n",
    "V1LHresp_val=V1LHresp_val[:,np.squeeze(significant_vox)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze encoding performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing encoding performance\n",
      "\n",
      "sigfniciant voxels: 469 / 490 at alpha = 0.00025\n",
      "encoding performance:  0.0379735022429  (mean R)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## Analyze encoding performance\n",
    "#Get num of significant voxels\n",
    "print('Analyzing encoding performance')\n",
    "print '\\nsigfniciant voxels:',n_vox-np.sum(em.rm.H_0),'/', n_vox, 'at alpha =',em.rm.alpha\n",
    "\n",
    "\n",
    "## Row-wise Correlation Coefficient calculation for two 2D arrays:\n",
    "def corr2_coeff(A,B):\n",
    "    # Rowwise mean of input arrays & subtract from input arrays themeselves\n",
    "    A_mA = A - A.mean(1)[:,None]\n",
    "    B_mB = B - B.mean(1)[:,None]\n",
    "    # Sum of squares across rows\n",
    "    ssA = (A_mA**2).sum(1);\n",
    "    ssB = (B_mB**2).sum(1);\n",
    "    # Finally get corr coeff\n",
    "    return np.dot(A_mA,B_mB.T)/np.sqrt(np.dot(ssA[:,None],ssB[None]))\n",
    "    \n",
    "# Get prediction / ground truth voxel correlations\n",
    "R = np.diagonal(corr2_coeff(V1LHresp_val.T,V1LHresp_val_hat[0].T))\n",
    "\n",
    "\n",
    "if V1LHresp_val_hat[0].shape[1]==0:\n",
    "    print 'no responses can be predicted at this alpha setting'\n",
    "else:\n",
    "    print 'encoding performance: ',np.mean(R),' (mean R)'\n",
    "    # Plot encoding performance Pyplot \n",
    "    fig = plt.figure()\n",
    "    plt.plot(np.arange(len(R))+1,sorted(R, reverse=True))\n",
    "    fig.suptitle('encoding performance')\n",
    "    plt.xlabel('voxel')\n",
    "    yLab=plt.ylabel('R')\n",
    "    yLab.set_rotation(0)\n",
    "    plt.ylim(-1, 1)\n",
    "    plt.xscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze identification performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "identification performance:  241.684701493 / 536 (mean ranking)\n"
     ]
    }
   ],
   "source": [
    "# Get identification accuracy\n",
    "dPoints=V1LHresp_val.shape[0]\n",
    "ranking=np.zeros(dPoints)\n",
    "for i in range(0,dPoints):\n",
    "    C=corr2_coeff (np.expand_dims(V1LHresp_val_hat[0][i,:],axis=1).T ,  V1LHresp_val)\n",
    "    ranking[i]=np.sum(C>C[:,i])+1\n",
    "print 'identification performance: ',np.mean(ranking),'/',V1LHresp_val.shape[0], '(mean ranking)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
